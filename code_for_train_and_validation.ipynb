{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list containing the image path & class label of all train data images\n",
    "json_path_train = '/Users/asmitasengupta/finproj/image_data_train.json'\n",
    "json_path_val = '/Users/asmitasengupta/finproj/image_data_val.json'\n",
    "#image_path = '/Users/asmitasengupta/Downloads/Birds_25/train'\n",
    "\n",
    "input_data = []\n",
    "val_data = []\n",
    "with open(json_path_train, 'r') as json_file:\n",
    "    input_data = json.load(json_file)\n",
    "\n",
    "with open(json_path_val, 'r') as json_file:\n",
    "    val_data = json.load(json_file)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose computation device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "\t\tdef __init__(self, list_image_path, list_txt):\n",
    "\t\t\t\n",
    "\t\t\tself.image_path = list_image_path  # Initialize image paths and corresponding texts\n",
    "\t\t\tself.title = clip.tokenize(list_txt)  # Tokenize text using CLIP's tokenizer\n",
    "\t\t\n",
    "\t\tdef __len__(self):\n",
    "\t\t\treturn len(self.title)  # Define the length of the dataset\n",
    "\n",
    "\t\tdef __getitem__(self, idx):     # Get an item from the dataset\n",
    "\t\t\timage = preprocess(Image.open(self.image_path[idx]))  # Preprocess image using CLIP's preprocessing function\n",
    "\t\t\ttitle = self.title[idx]\n",
    "\t\t\treturn image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training\n",
    "list_image_path = []\n",
    "list_txt = []\n",
    "for item in input_data:\n",
    "  img_path = item['Image_path']\n",
    "  caption = item['Class_label']\n",
    "  list_image_path.append(img_path)\n",
    "  list_txt.append(caption)\n",
    "\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "train_dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "#for validation\n",
    "list_image_path_v = []\n",
    "list_txt_v = []\n",
    "for item in val_data:\n",
    "  img_path = item['Image_path']\n",
    "  caption = item['Class_label']\n",
    "  list_image_path_v.append(img_path)\n",
    "  list_txt_v.append(caption)\n",
    "\n",
    "val_dataset = image_title_dataset(list_image_path_v, list_txt_v)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format so that the model loads in the provided memory.\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the device is set to CPU\n",
    "if device == \"cpu\":\n",
    "    model.float()  # Convert the model's parameters to float if using CPU\n",
    "\n",
    "# Prepare the optimizer with specific hyperparameters\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6 ,weight_decay=0.2) \n",
    "\n",
    "# Loss function for images\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "\n",
    "# Loss function for text\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train the model\n",
    "\n",
    "num_epochs = 10 # No. of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    \n",
    "    # Iterate through the batches in the training data\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()  # Zero out gradients for the optimizer\n",
    "        images, texts = batch  # Extract images and texts from the batch\n",
    "        print(device)  # Print the current device (CPU or GPU)\n",
    "        \n",
    "        # Move images and texts to the specified device (CPU or GPU)\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute the loss\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "        # Backward pass and update the model's parameters\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # If the device is CPU, directly update the model\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            # Convert model's parameters to FP32 format, update, and convert back\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        # Update the progress bar with the current epoch and loss\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(), filepath = 'models/trained_model.pth')  #saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluating model performance on validation data\n",
    "\n",
    "# Check if CUDA (GPU) is available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the CLIP model and preprocessing pipeline\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load the state dictionary of the trained model\n",
    "#saved_model_state_dict = torch.load(\"models/trained_model.pth\", map_location=device)\n",
    "\n",
    "# Update the model's state dictionary with the saved state dictionary\n",
    "#model.load_state_dict(saved_model_state_dict)\n",
    "\n",
    "#All class labels of Birds_25 dataset\n",
    "birds = [\n",
    "\"Asian-Green-Bee-Eater\", \"Common-Rosefinch\", \"Hoopoe\", \"Indian-Roller\", \"Rufous-Treepie\",\n",
    "\"Brown-Headed-Barbet\", \"Common-Tailorbird\", \"House-Crow\", \"Jungle-Babbler\", \"Sarus-Crane\",\n",
    "\"Cattle-Egret\", \"Coppersmith-Barbet\", \"Indian-Grey-Hornbill\", \"Northern-Lapwing\", \"White-Breasted-Kingfisher\",\n",
    "\"Common-Kingfisher\", \"Forest-Wagtail\", \"Indian-Peacock\", \"Red-Wattled-Lapwing\", \"White-Breasted-Waterhen\",\n",
    "\"Common-Myna\", \"Gray-Wagtail\", \"Indian-Pitta\", \"Ruddy-Shelduck\", \"White-Wagtail\"]\n",
    "\n",
    "birds.sort(reverse=True)\n",
    "#print(birds)\n",
    "\n",
    "class_dict = {}\n",
    "for _, b in enumerate(birds):\n",
    "    class_dict[b]= _\n",
    "\n",
    "#Index of the input data you want to analyze\n",
    "index_ = 500\n",
    "predicted_labels, true_labels=[] , []\n",
    "for index_ in range(7500):\n",
    "\n",
    "    image_json = val_data[index_]\n",
    "\n",
    "    #path to the image file\n",
    "    image_path = image_json['Image_path']\n",
    "\n",
    "    #Get the class label of the image\n",
    "    image_class = image_json['Class_label']\n",
    "\n",
    "    #Preprocess the image and move it to the appropriate device (CPU/GPU)\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    #Tokenize and move the bird item names to the appropriate device\n",
    "    text = torch.cat([clip.tokenize(c) for c in birds]).to(device)\n",
    "\n",
    "    #Perform inference\n",
    "    with torch.no_grad():\n",
    "        #Encode image and text\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        \n",
    "        #Calculate similarity scores between image and text\n",
    "        logits_per_image, logits_per_text = model(image, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    #Normalize image and text features\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    #Calculate similarity scores\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(1)\n",
    "\n",
    "    true_labels.append(class_dict[image_class])\n",
    "    predicted_labels.append(indices)\n",
    "    \n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "class_wise_accuracy = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "print(\"Class-wise Accuracy:\")\n",
    "for i, acc in enumerate(class_wise_accuracy):\n",
    "    print(f\"Class {i}: {acc:.4f}\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
